import streamlit as st
import os
import cv2
import time
import torch
import av
import numpy as np
import copy
from ultralytics import YOLO
from tempfile import NamedTemporaryFile

import torch
print("torch version:", torch.__version__)
print("cuda available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("cuda device count:", torch.cuda.device_count())
    try:
        print("device name:", torch.cuda.get_device_name(0))
    except Exception as e:
        print("device name error:", e)


# S·ª≠ d·ª•ng streamlit-webrtc cho webcam realtime
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, WebRtcMode

# --- IMPORT C√ÅC MODULE CUSTOM C·ª¶A B·∫†N ---
# Ch√∫ √Ω: C√°c file n√†y ph·∫£i n·∫±m trong c√πng th∆∞ m·ª•c ho·∫∑c trong PYTHONPATH
try:
    from utils.draw_utils import draw_text_pil
    from utils.video_utils import compute_fps
    from rep_counter import (
        count_squat, count_pushup, count_plank, count_situp
    )
    from form_rules import (
        evaluate_squat, evaluate_pushup, evaluate_plank, evaluate_situp
    )
    # T·∫Øt voice_player v√¨ server-side voice playback r·∫•t kh√≥ khƒÉn trong Streamlit
    # import voice_player
    # print("ƒê√£ t·∫£i c√°c module h·ªó tr·ª£.")
except ImportError as e:
    st.error(f"L·ªói t·∫£i module t√πy ch·ªânh: {e}")
    st.info("Vui l√≤ng ƒë·∫£m b·∫£o c√°c file 'rep_counter.py', 'form_rules.py', v√† 'utils/' t·ªìn t·∫°i trong th∆∞ m·ª•c.")


    # ƒê·ªãnh nghƒ©a dummy functions ƒë·ªÉ app v·∫´n ch·∫°y (ch·ªâ v·∫Ω, kh√¥ng t√≠nh to√°n)
    def dummy_func(*args, **kwargs):
        return (0, "up", 0)


    def dummy_eval(*args, **kwargs):
        return (0, "Module l·ªói!", "neutral")


    count_squat, count_pushup, count_plank, count_situp = [dummy_func] * 4
    evaluate_squat, evaluate_pushup, evaluate_plank, evaluate_situp = [dummy_eval] * 4

# -----------------------------
# ‚öôÔ∏è C·∫•u h√¨nh v√† Caching Model
# -----------------------------
IMG_SIZE = 480
CONF_THRESHOLD = 0.5
DRAW_EVERY_N_FRAMES = 3
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Target output resolution (1080p)
TARGET_W, TARGET_H = 1920, 1080

# X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n font (∆∞u ti√™n Noto Sans c√≥ h·ªó tr·ª£ ti·∫øng Vi·ªát)
FONTS_DIR = os.path.join(BASE_DIR, "fonts")
os.makedirs(FONTS_DIR, exist_ok=True)
FONT_PATH = os.path.join(FONTS_DIR, "NotoSans-Regular.ttf")
if not os.path.exists(FONT_PATH):
    # fallback to Roboto if Noto not found
    FONT_PATH = os.path.join(FONTS_DIR, "Roboto-Regular.ttf")
if not os.path.exists(FONT_PATH):
    st.warning("Kh√¥ng t√¨m th·∫•y font Unicode h·ªó tr·ª£ ti·∫øng Vi·ªát trong ./fonts/. S·∫Ω d√πng font m·∫∑c ƒë·ªãnh c·ªßa OpenCV.")
    FONT_PATH = None


@st.cache_resource
def load_yolo_model():
    """T·∫£i v√† cache model YOLO Pose."""
    # Use canonical device strings accepted by ultralytics/torch
    device = "cuda" if torch.cuda.is_available() else "cpu"
    st.info(f"ƒêang t·∫£i model tr√™n device: {device} (cuda available: {torch.cuda.is_available()})")
    try:
        model = YOLO("yolo11n-pose.pt")
        model.conf = CONF_THRESHOLD
        # move model to device and set model args if needed
        try:
            model.to(device)
        except Exception as _:
            # ultralytics YOLO handles device via predict parameter; fallback ok
            pass
        return model, device
    except Exception as e:
        st.error(f"L·ªói t·∫£i model: {e}")
        return None, device


model, device = load_yolo_model()

# -----------------------------
# üß© ƒêƒÉng k√Ω B√†i t·∫≠p (ƒê·ªìng b·ªô v·ªõi code g·ªëc)
# -----------------------------
exercise_registry = {
    "squat": {
        "counter_func": count_squat,
        "form_func": evaluate_squat,
        "state": {"stage": "up", "counter": 0, "prev_angle": 160, "direction": "up"},
    },
    "pushup": {
        "counter_func": count_pushup,
        "form_func": evaluate_pushup,
        "state": {"stage": "up", "counter": 0, "prev_angle": 160, "direction": "up"},
    },
    "plank": {
        "counter_func": count_plank,
        "form_func": evaluate_plank,
        "state": {"good_time": 0, "bad_time": 0, "is_good": False, "elapsed": 0.0},
    },
    "situp": {
        "counter_func": count_situp,
        "form_func": evaluate_situp,
        "state": {"stage": "down", "counter": 0, "prev_angle": 140, "direction": "down"},
    },
}


# -----------------------------
# üé• CLASS X·ª¨ L√ù VIDEO REALTIME (cho Webcam)
# -----------------------------

class PoseProcessor(VideoProcessorBase):
    """X·ª≠ l√Ω t·ª´ng khung h√¨nh video t·ª´ webcam."""

    def __init__(self, exercise_key, model, device):
        self.model = model
        self.device = device
        self.exercise_key = exercise_key

        if exercise_key not in exercise_registry:
            raise ValueError(f"B√†i t·∫≠p {exercise_key} kh√¥ng h·ª£p l·ªá.")

        self.counter_func = exercise_registry[exercise_key]["counter_func"]
        self.form_func = exercise_registry[exercise_key]["form_func"]

        # Deep copy state ƒë·ªÉ m·ªói phi√™n web c√≥ state ri√™ng
        self.state = copy.deepcopy(exercise_registry[exercise_key]["state"])

        self.prev_time = time.time()
        self.frame_idx = 0
        self.last_annotated = None
        self.FONT_PATH = FONT_PATH
        self.start_time = time.time()

    def process_frame(self, img):
        """Logic x·ª≠ l√Ω ch√≠nh (t∆∞∆°ng t·ª± v√≤ng l·∫∑p while trong code g·ªëc)"""

        # 1. Resize cho inference (gi·ªØ IMG_SIZE ƒë·ªÉ model nhanh)
        infer_frame = cv2.resize(img, (IMG_SIZE, IMG_SIZE))

        results = self.model.predict(infer_frame,
                                     verbose=False,
                                     conf=CONF_THRESHOLD,
                                     device=self.device,
                                     batch=1)
        res = results[0]

        # 2. V·∫Ω annotation (d√πng plot t·ª´ result; k√≠ch th∆∞·ªõc d·ª±a tr√™n infer_frame)
        if self.frame_idx % DRAW_EVERY_N_FRAMES == 0:
            annotated = res.plot()
            self.last_annotated = annotated
        else:
            annotated = self.last_annotated if self.last_annotated is not None else infer_frame.copy()

        counter = 0
        stage = "..."
        angle = 0
        feedback = "..."
        form_color = (255, 255, 255)

        # 3. X·ª≠ l√Ω Keypoints
        if res.keypoints is not None and len(res.keypoints.xy) > 0:
            kps = res.keypoints.xy[0].tolist()

            try:
                # C·∫≠p nh·∫≠t elapsed time cho Plank
                if self.exercise_key == "plank":
                    self.state['elapsed'] = time.time() - self.start_time

                counter, stage, angle = self.counter_func(kps, self.state)
                form_score, feedback, tone = self.form_func(kps, annotated, stage, counter)

                form_color = (0, 255, 0) if tone == "positive" or tone == "good" else (0, 0, 255)
            except Exception as e:
                # X·ª≠ l√Ω l·ªói n·∫øu logic b·ªã crash
                feedback = f"L·ªói x·ª≠ l√Ω: {e.__class__.__name__}"
                form_color = (0, 165, 255)  # M√†u cam

        else:
            feedback = "Kh√¥ng ph√°t hi·ªán ng∆∞·ªùi"

        # 4. T√≠nh FPS
        fps, self.prev_time = compute_fps(self.prev_time)

        # 5. Overlay Text
        if self.exercise_key == "plank":
            elapsed = self.state.get("elapsed", 0.0)
            lines = [
                (f"B√†i t·∫≠p: {self.exercise_key.upper()}", (255, 105, 180)),
                (f"Th·ªùi gian gi·ªØ: {elapsed:.1f}s", (255, 215, 0)),
                (f"T∆∞ th·∫ø: {'Chu·∫©n' if self.state.get('is_good') else 'Ch∆∞a ƒë√∫ng'}", (255, 255, 255)),
            ]
        else:
            lines = [
                (f"B√†i t·∫≠p: {self.exercise_key.upper()}", (255, 105, 180)),
                (f"S·ªë l·∫ßn: {self.state.get('counter', 0)}", (255, 215, 0)),
                (f"Tr·∫°ng th√°i: {stage}", (255, 255, 255)),
            ]

        lines.extend([
            (f"G√≥c: {int(angle)}¬∞", (144, 238, 144)),
            (f"ƒê√°nh gi√°: {feedback}", form_color),
            (f"FPS: {fps:.1f}", (200, 200, 200)),
        ])

        annotated = draw_text_pil(annotated, lines, font_path=self.FONT_PATH, font_scale=26, pos=(20, 20))

        # 3. Resize annotated -> TARGET (1080p) before returning (preserve aspect by scaling then padding)
        try:
            h, w = annotated.shape[:2]
            if w == 0 or h == 0:
                final = cv2.resize(infer_frame, (TARGET_W, TARGET_H), interpolation=cv2.INTER_LINEAR)
            else:
                scale = min(TARGET_W / w, TARGET_H / h)
                new_w = max(1, int(w * scale))
                new_h = max(1, int(h * scale))
                interp = cv2.INTER_AREA if scale < 1.0 else cv2.INTER_LINEAR
                resized = cv2.resize(annotated, (new_w, new_h), interpolation=interp)
                top = (TARGET_H - new_h) // 2
                bottom = TARGET_H - new_h - top
                left = (TARGET_W - new_w) // 2
                right = TARGET_W - new_w - left
                final = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0,0,0])
        except Exception:
            final = cv2.resize(annotated, (TARGET_W, TARGET_H), interpolation=cv2.INTER_LINEAR)

        self.frame_idx += 1
        return final

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        """Callback nh·∫≠n khung h√¨nh t·ª´ webrtc"""
        img = frame.to_ndarray(format="bgr24")
        processed_img = self.process_frame(img)

        # Chuy·ªÉn l·∫°i v·ªÅ AV VideoFrame ƒë·ªÉ hi·ªÉn th·ªã (processed_img ƒë·∫£m b·∫£o l√† TARGET size)
        return av.VideoFrame.from_ndarray(processed_img, format="bgr24")


# -----------------------------
# üåê GIAO DI·ªÜN STREAMLIT CH√çNH
# -----------------------------

st.title("üèãÔ∏è AI Fitness Tracker")
st.sidebar.title("C·∫•u h√¨nh")

# 1. Ch·ªçn B√†i t·∫≠p
exercise_choice = st.sidebar.selectbox(
    "Ch·ªçn b√†i t·∫≠p:",
    list(exercise_registry.keys()),
    key='exercise'
)

# 2. Ch·ªçn Ch·∫ø ƒë·ªô
mode = st.sidebar.radio(
    "Ch·ªçn ch·∫ø ƒë·ªô ho·∫°t ƒë·ªông:",
    ["Webcam Realtime", "Ph√¢n t√≠ch Video Upload"]
)

if model is None:
    st.error("Kh√¥ng th·ªÉ kh·ªüi t·∫°o model. Vui l√≤ng ki·ªÉm tra file yolo11n-pose.pt.")
elif mode == "Webcam Realtime":

    st.header("üî¥ Webcam Realtime (S·ª≠ d·ª•ng `streamlit-webrtc`)")

    # Y√™u c·∫ßu camera client capture ·ªü 1080p (n·∫øu tr√¨nh duy·ªát/thi·∫øt b·ªã h·ªó tr·ª£)
    media_constraints = {
        "video": {
            "width": {"ideal": TARGET_W},
            "height": {"ideal": TARGET_H},
            "frameRate": {"ideal": 30}
        },
        "audio": False
    }

    ctx = webrtc_streamer(
        key="webcam_processor",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration={
            "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
        },
        video_processor_factory=lambda: PoseProcessor(exercise_choice, model, device),
        media_stream_constraints=media_constraints,
        async_processing=True,
    )

    if ctx.state.playing:
        st.success(f"ƒêang ph√¢n t√≠ch t∆∞ th·∫ø {exercise_choice.upper()}...")
        # (Th√¥ng tin v√† feedback s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã tr·ª±c ti·∫øp tr√™n lu·ªìng video)


elif mode == "Ph√¢n t√≠ch Video Upload":

    st.header("‚¨ÜÔ∏è Ph√¢n t√≠ch Video ƒê√£ T·∫£i L√™n")

    uploaded_file = st.file_uploader(
        "T·∫£i l√™n m·ªôt file video (.mp4, .mov)",
        type=['mp4', 'mov']
    )

    if uploaded_file is not None:

        # 1. L∆∞u file t·∫°m th·ªùi
        tfile = NamedTemporaryFile(delete=False)
        tfile.write(uploaded_file.read())
        tfile.close()
        video_path = tfile.name

        st.video(uploaded_file, format="video/mp4", start_time=0)

        start_button = st.button("B·∫Øt ƒë·∫ßu Ph√¢n t√≠ch Video")

        if start_button:
            st.info("ƒêang x·ª≠ l√Ω video... Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t th·ªùi gian t√πy thu·ªôc v√†o ƒë·ªô d√†i video.")

            # Kh·ªüi t·∫°o logic x·ª≠ l√Ω
            counter_func = exercise_registry[exercise_choice]["counter_func"]
            form_func = exercise_registry[exercise_choice]["form_func"]
            state = copy.deepcopy(exercise_registry[exercise_choice]["state"])

            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                st.error("Kh√¥ng th·ªÉ m·ªü video.")
            else:
                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                fps_input = cap.get(cv2.CAP_PROP_FPS)

                # Streamlit placeholder ƒë·ªÉ c·∫≠p nh·∫≠t frame li√™n t·ª•c
                frame_placeholder = st.empty()
                progress_bar = st.progress(0)

                prev_time = time.time()
                frame_idx = 0
                last_annotated = None
                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

                if exercise_choice == "plank":
                    state['start_time'] = time.time()  # Gi·∫£ ƒë·ªãnh start_time c·ªßa video l√† 0

                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break

                    frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))

                    # X·ª≠ l√Ω t∆∞∆°ng t·ª± nh∆∞ trong PoseProcessor
                    results = model.predict(frame, verbose=False, conf=CONF_THRESHOLD, device=device, batch=1)
                    res = results[0]

                    if frame_idx % DRAW_EVERY_N_FRAMES == 0:
                        annotated = res.plot()
                        last_annotated = annotated
                    else:
                        annotated = last_annotated if last_annotated is not None else frame.copy()

                    counter, stage, angle, feedback, form_color = 0, "...", 0, "...", (255, 255, 255)

                    if res.keypoints is not None and len(res.keypoints.xy) > 0:
                        kps = res.keypoints.xy[0].tolist()

                        if exercise_choice == "plank":
                            # T√≠nh th·ªùi gian th·ª±c trong video
                            time_in_video = frame_idx / fps_input
                            state['elapsed'] = time_in_video

                        counter, stage, angle = counter_func(kps, state)
                        form_score, feedback, tone = form_func(kps, annotated, stage, counter)
                        form_color = (0, 255, 0) if tone == "positive" or tone == "good" else (0, 0, 255)
                    else:
                        feedback = "Kh√¥ng ph√°t hi·ªán ng∆∞·ªùi"

                    # T√≠nh FPS (d·ª±a tr√™n t·ªëc ƒë·ªô x·ª≠ l√Ω c·ªßa m√°y t√≠nh)
                    fps, prev_time = compute_fps(prev_time)

                    # Overlay Text
                    if exercise_choice == "plank":
                        elapsed = state.get("elapsed", 0.0)
                        lines = [
                            (f"B√†i t·∫≠p: {exercise_choice.upper()}", (255, 105, 180)),
                            (f"Th·ªùi gian gi·ªØ: {elapsed:.1f}s", (255, 215, 0)),
                            (f"T∆∞ th·∫ø: {'Chu·∫©n' if state.get('is_good') else 'Ch∆∞a ƒë√∫ng'}", (255, 255, 255)),
                        ]
                    else:
                        lines = [
                            (f"B√†i t·∫≠p: {exercise_choice.upper()}", (255, 105, 180)),
                            (f"S·ªë l·∫ßn: {state.get('counter', 0)}", (255, 215, 0)),
                            (f"Tr·∫°ng th√°i: {stage}", (255, 255, 255)),
                        ]

                    lines.extend([
                        (f"G√≥c: {int(angle)}¬∞", (144, 238, 144)),
                        (f"ƒê√°nh gi√°: {feedback}", form_color),
                        (f"FPS: {fps:.1f} (X·ª≠ l√Ω)", (200, 200, 200)),
                    ])

                    annotated = draw_text_pil(annotated, lines, font_path=FONT_PATH, font_scale=26, pos=(20, 20))

                    # Resize annotated -> 1080p before display / writing
                    try:
                        annotated = cv2.resize(annotated, (TARGET_W, TARGET_H), interpolation=cv2.INTER_LINEAR)
                    except Exception:
                        pass

                    # Hi·ªÉn th·ªã tr√™n Streamlit
                    frame_placeholder.image(
                        annotated,
                        channels="BGR",
                        caption=f"Frame {frame_idx}/{total_frames}",
                        use_column_width=True
                    )

                    frame_idx += 1
                    progress_bar.progress(frame_idx / total_frames)

                cap.release()
                st.success(f"Ph√¢n t√≠ch video ho√†n t·∫•t. T·ªïng s·ªë l·∫ßn: {state.get('counter', 0)}.")
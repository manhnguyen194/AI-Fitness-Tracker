import os
# os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
import cv2
import time
import torch
from ultralytics import YOLO

# Import module h·ªó tr·ª£
from utils.draw_utils import draw_text_pil
from utils.video_utils import setup_window, compute_fps
from rep_counter import count_squat, count_pushup
from form_rules import evaluate_squat, evaluate_pushup

# -----------------------------
# ‚öôÔ∏è C·∫•u h√¨nh
# -----------------------------
EXERCISE =  "squat" # ho·∫∑c "squat"
VIDEO_REL = os.path.join("data", "raw", "squat_ok_01.mp4")
# file data/ n·∫±m b√™n trong src/, kh√¥ng ph·∫£i ·ªü project root -> kh√¥ng c·∫ßn ".."
VIDEO_PATH = os.path.normpath(os.path.join(os.path.dirname(__file__), VIDEO_REL))

# N·∫øu kh√¥ng t√¨m th·∫•y file th√¨ th√¥ng b√°o r√µ r√†ng v√† fallback sang webcam (0)
if not os.path.exists(VIDEO_PATH):
    print(f"‚ùå Video kh√¥ng t√¨m th·∫•y t·∫°i: {VIDEO_PATH}")
    print("‚ûú ƒê·∫∑t file v√†o data/raw/ ho·∫∑c ƒë·ªïi VIDEO_PATH. T·ª± ƒë·ªông chuy·ªÉn sang webcam (0).")
    VIDEO_PATH = 0

print(f"‚ñ∂Ô∏è S·ª≠ d·ª•ng video/webcam: {VIDEO_PATH}")

FONT_PATH = os.path.join(os.path.dirname(__file__), "..", "fonts", "Roboto.ttf")

# Add/modify these configurations at the top
BATCH_SIZE = 1
IMG_SIZE = 640  # or 480 for faster processing
DRAW_EVERY_N_FRAMES = 3  # Increase to 3 or 4 for higher FPS

# Th√™m c·∫•u h√¨nh sau ph·∫ßn CONFIG
CONF_THRESHOLD = 0.5     # L·ªçc b·ªõt detection c√≥ ƒë·ªô tin c·∫≠y th·∫•p

# -----------------------------
# üöÄ Kh·ªüi t·∫°o model (v·ªõi GPU n·∫øu c√≥)
# -----------------------------
device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(f"‚ñ∂Ô∏è Device: {device}")

# Debug CUDA status
print("\n=== üîç GPU/CUDA Status ===")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"Current device: {torch.cuda.current_device()}")
    print(f"Device name: {torch.cuda.get_device_name()}")
    print(f"Device memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    print("‚ö†Ô∏è CUDA kh√¥ng kh·∫£ d·ª•ng - model ƒëang ch·∫°y tr√™n CPU")
print("=====================\n")

# T·ªëi ∆∞u th√™m cho CUDA
if device.startswith("cuda"):
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

# Modify model initialization
model = YOLO("yolo11n-pose.pt")
model.conf = CONF_THRESHOLD
model.to(device)

cap = cv2.VideoCapture(VIDEO_PATH)

if not cap.isOpened():
    print("‚ùå Kh√¥ng th·ªÉ m·ªü video ho·∫∑c webcam:", VIDEO_PATH)
    exit()

state = {"stage": "up", "counter": 0}
prev_time = time.time()
frame_idx = 0
print("‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu. Nh·∫•n 'q' ƒë·ªÉ tho√°t.")

exercise_registry = {
    "squat": {
        "counter_func": count_squat,
        "form_func": evaluate_squat,
        "state": {"stage": "up", "counter": 0, "prev_angle": 160, "direction": "up"},
    },
    "pushup": {
        "counter_func": count_pushup,
        "form_func": evaluate_pushup,
        "state": {"stage": "up", "counter": 0, "prev_angle": 160, "direction": "up"},
    },
    # Th√™m b√†i t·∫≠p m·ªõi ·ªü ƒë√¢y:
    # "lunges": {"counter_func": count_lunges, "form_func": evaluate_lunges, "state": {...}}
}

if EXERCISE not in exercise_registry:
    raise ValueError(f"‚ùå B√†i t·∫≠p '{EXERCISE}' ch∆∞a ƒë∆∞·ª£c ƒëƒÉng k√Ω trong exercise_registry!")

counter_func = exercise_registry[EXERCISE]["counter_func"]
form_func = exercise_registry[EXERCISE]["form_func"]
state = exercise_registry[EXERCISE]["state"]

# -----------------------------
# üîÅ V√≤ng l·∫∑p ch√≠nh
# -----------------------------
while True:
    ret, frame = cap.read()
    if not ret:
        print("üé¨ H·∫øt video ho·∫∑c l·ªói ƒë·ªçc frame.")
        break
    frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))

    # Use model.predict instead of direct call for better GPU utilization
    results = model.predict(frame, 
                       verbose=False,
                       conf=CONF_THRESHOLD,
                       device=device,
                       batch=BATCH_SIZE)
    res = results[0]
    
    # Ch·ªâ v·∫Ω annotation m·ªói N frame
    if frame_idx % DRAW_EVERY_N_FRAMES == 0:
        annotated = res.plot()
    else:
        annotated = frame.copy()

    counter = 0
    stage = "up"
    angle = 0
    feedback = "..."

    # N·∫øu c√≥ keypoints ‚Üí x·ª≠ l√Ω
    if res.keypoints is not None and len(res.keypoints.xy) > 0:
        kps = res.keypoints.xy[0].tolist()

        # G·ªçi h√†m ƒë·∫øm v√† ƒë√°nh gi√° form t∆∞∆°ng ·ª©ng b√†i t·∫≠p
        counter, stage, angle = counter_func(kps, state)
        form_score, feedback = form_func(kps, annotated, stage, counter)

    # FPS
    fps, prev_time = compute_fps(prev_time)

    # -----------------------------
    # üñºÔ∏è Overlay text
    # -----------------------------
    form_color = (0, 255, 0) if "t·ªët" in feedback or "chu·∫©n" in feedback else (255, 255, 0) if "c√≥ th·ªÉ" in feedback else (255, 80, 80)
    lines = [
        (f"S·ªë l·∫ßn: {counter}", (255, 215, 0)),
        (f"Tr·∫°ng th√°i: {stage}", (255, 255, 255)),
        (f"G√≥c: {int(angle)}¬∞", (144, 238, 144)),
        (f"ƒê√°nh gi√°: {feedback}", form_color),
        (f"FPS: {fps:.1f}", (200, 200, 200)),
    ]

    annotated = draw_text_pil(annotated, lines, font_path=FONT_PATH, font_scale=26, pos=(20, 20))

    # -----------------------------
    # üñ•Ô∏è Hi·ªÉn th·ªã video auto-scale
    # -----------------------------
    if frame_idx == 0:
        setup_window("AI Fitness Tracker", annotated, max_height=720)

    cv2.imshow("AI Fitness Tracker", annotated)
    frame_idx += 1

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

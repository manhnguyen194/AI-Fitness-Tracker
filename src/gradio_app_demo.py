#!/usr/bin/env python3
"""
gradio_app_demo.py (ƒê√É S·ª¨A L·ªñI HO√ÄN CH·ªàNH)

- FIX 1 (L·ªói k√©o gi√£n): S·ª≠a l·ªói g√µ nh·∫ßm 'out_h, out_h' th√†nh 'out_h, out_w'
  trong h√†m process_range (d√≤ng 535).
- FIX 7 (L·ªói Webcam): S·ª≠a l·ªói 'UnboundLocalError' b·∫±ng c√°ch
  ƒë·ªïi 'frame_count' th√†nh 'frame_idx' (d√≤ng 373-380).
- FIX 8 (L·ªói Video): S·ª≠a l·ªói 'KeyError' b·∫±ng c√°ch g√°n k·∫øt qu·∫£
  v√†o 'local_state[exercise]' thay v√¨ 'local_state' (d√≤ng 515).
- FIX 9 (Y√™u c·∫ßu): ƒê√É B·∫¨T L·∫†I (un-comment) d√≤ng "ƒê√°nh gi√°: {feedback}"
  trong _process_frame_logic (d√≤ng 310 v√† 318).
- FIX 10 (T∆∞∆°ng th√≠ch): S·ª≠a l·ªói g·ªçi 'form_func' ƒë·ªÉ kh·ªõp v·ªõi
  file form_rules.py m·ªõi (truy·ªÅn 5 tham s·ªë, bao g·ªìm 'state').
"""

import os
import gradio as gr
from ultralytics import YOLO
import cv2
import torch
import time
import threading
import numpy as np
import tempfile
import traceback
from pathlib import Path
from typing import Optional, Callable
import shutil
import atexit

# === Import logic m·ªõi ===
from rep_counter import count_squat, count_pushup, count_plank, count_situp
# üõ†Ô∏è S·ª¨A: Import c√°c h√†m ƒë√°nh gi√° c·ª• th·ªÉ
from form_rules import evaluate_squat, evaluate_pushup, evaluate_plank, evaluate_situp
from utils.draw_utils import draw_text_pil
from utils.video_utils import compute_fps

# === C√°c bi·∫øn to√†n c·ª•c cho Webcam Ngo√†i (t·ª´ code c≈©) ===
EXTERNAL_THREAD = None
EXTERNAL_STOP = threading.Event()
# (MODEL v√† MODEL_LOCK s·∫Ω d√πng chung v·ªõi logic m·ªõi)

# === C√°c bi·∫øn to√†n c·ª•c cho Video 3 ph·∫ßn (t·ª´ code c≈©) ===
BG_TASK = {"thread": None, "status": "idle", "part1_path": None, "final_path": None, "tmp_dir": None, "error": None}

# === C·∫•u h√¨nh chung (t·ª´ code m·ªõi) ===
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    print("=== üîç GPU/CUDA Status ===")
    print(f"CUDA available: {torch.cuda.is_available()}")
    print(f"Current device: {torch.cuda.current_device()}")
    print(f"Device name: {torch.cuda.get_device_name(0)}")
    print(f"Device memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print("=====================")
    # Optimize CUDA backends for real-time
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
else:
    device = torch.device("cpu")
    print("‚ö†Ô∏è CUDA not available. Running on CPU.")

try:
    import importlib
    import importlib.util
    spec = importlib.util.find_spec("moviepy.editor")
    if spec is not None:
        moviepy = importlib.import_module("moviepy.editor")
        ImageSequenceClip = getattr(moviepy, "ImageSequenceClip", None)
        USE_MOVIEPY = ImageSequenceClip is not None
    else:
        ImageSequenceClip = None
        USE_MOVIEPY = False
except Exception:
    ImageSequenceClip = None
    USE_MOVIEPY = False

# Global model and stream state for webcam processing
GLOBAL_MODEL = None
MODEL_LOCK = threading.Lock() # Th√™m lock t·ª´ logic m·ªõi
GLOBAL_CONF = 0.5

# === C·∫¨P NH·∫¨T: ƒê·ªìng b·ªô t·ª´ pose_extractor.py ===
GLOBAL_IMG_SIZE = 640  # K√≠ch th∆∞·ªõc input cho model (thay v√¨ 384)
INFER_EVERY_N = 3 # ƒê·ªìng b·ªô v·ªõi DRAW_EVERY_N_FRAMES (thay v√¨ 2)
# === K·∫æT TH√öC C·∫¨P NH·∫¨T ===

GLOBAL_USE_HALF = bool(torch.cuda.is_available())  # d√πng FP16 n·∫øu c√≥ CUDA
DISPLAY_MAX_WIDTH = 1280 # Cho c·ª≠a s·ªï webcam ngo√†i
CAP_DEVICE_INDEX = 0

# ƒê∆∞·ªùng d·∫´n font (t·ª´ logic m·ªõi)
font_path = Path(__file__).parent.parent / "fonts" / "Roboto.ttf"
if not font_path.exists():
    # Fallback n·∫øu kh√¥ng t√¨m th·∫•y font
    try:
        # Th·ª≠ arial tr√™n Windows
        font_path = "arial.ttf"
        from PIL import ImageFont
        ImageFont.truetype(font_path, 10)
    except Exception:
        # Fallback cu·ªëi c√πng
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y font 'Roboto.ttf' ho·∫∑c 'arial.ttf'. Overlay vƒÉn b·∫£n c√≥ th·ªÉ b·ªã l·ªói.")
        font_path = None


def get_model(weights_path: Optional[str] = "yolo11n-pose.pt"):
    """
    H·ª£p nh·∫•t t·ª´ _get_model (m·ªõi) v√† get_model (c≈©).
    Lazily initialize, d√πng lock, h·ªó tr·ª£ custom weights, v√† warm-up.
    """
    global GLOBAL_MODEL, MODEL_LOCK
    with MODEL_LOCK:
        if GLOBAL_MODEL is None:
            try:
                m = YOLO(weights_path)
            except Exception as e:
                print(f"Error loading {weights_path}: {e}. Fallback to default.")
                m = YOLO("yolo11n-pose.pt")
            m.conf = GLOBAL_CONF
            try:
                m.to(device)
            except Exception:
                pass
            # Fuse Conv+BN ƒë·ªÉ tƒÉng t·ªëc n·∫øu ƒë∆∞·ª£c h·ªó tr·ª£
            try:
                m.fuse()
            except Exception:
                pass
            # Warm-up
            try:
                import numpy as _np
                dummy = _np.zeros((GLOBAL_IMG_SIZE, GLOBAL_IMG_SIZE, 3), dtype=_np.uint8)
                with torch.inference_mode():
                    _ = m.predict(dummy, verbose=False, device=str(device), imgsz=GLOBAL_IMG_SIZE, half=GLOBAL_USE_HALF, max_det=1, conf=GLOBAL_CONF)
            except Exception:
                pass
            GLOBAL_MODEL = m
        return GLOBAL_MODEL

# L·∫•y h√†m tr√≠ch xu·∫•t KPS t·ªët nh·∫•t (t·ª´ code local_webcam trong file m·ªõi)
def safe_extract_kps(res):
    """
    Tr√≠ch xu·∫•t keypoints (robust) t·ª´ k·∫øt qu·∫£ YOLO.
    (ƒê·ªïi t√™n t·ª´ safe_get_kps_from_res_local)
    """
    try:
        r0 = res[0]
        if hasattr(r0, "keypoints") and r0.keypoints is not None:
            # üõ†Ô∏è S·ª¨A (FIX 4): Logic n√†y ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ ch·ªâ l·∫•y 'xy'
            # v√† ng∆∞·ªùi c√≥ conf cao nh·∫•t, kh·ªõp v·ªõi logic mong mu·ªën.
            if not res or not res.keypoints or not res.boxes:
                return None
            
            confs = res.boxes.conf.cpu().numpy() if res.boxes.conf is not None else []
            if len(confs) == 0:
                return None
            
            max_idx = np.argmax(confs)
            if confs[max_idx] < 0.6:  # Threshold ƒë·ªÉ b·ªè low-conf
                return None

            kps_tensor = getattr(res.keypoints, "xy", None)
            if kps_tensor is None:
                return None
                
            kps_list = kps_tensor.cpu().numpy()
            if max_idx >= len(kps_list):
                return None # Index out of bounds
                
            kps = kps_list[max_idx] # L·∫•y KPS c·ªßa ng∆∞·ªùi c√≥ conf cao nh·∫•t
            
            if kps.ndim == 2 and kps.shape[0] > 0 and kps.shape[1] >= 2:
                 return kps[:, :2]  # Tr·∫£ array cho d·ªÖ d√πng
    except Exception:
        pass
    return None

# Registry b√†i t·∫≠p (t·ª´ code m·ªõi)
EXERCISE_REGISTRY = {
    "Squat": {
        "counter_func": count_squat,
        "form_func": evaluate_squat, # üõ†Ô∏è S·ª¨A: D√πng h√†m evaluate_squat tr·ª±c ti·∫øp
        "state": {"stage": "up", "counter": 0, "prev_angle": 160, "direction": "up"},
    },
    "Push-up": {
        "counter_func": count_pushup,
        "form_func": evaluate_pushup, # üõ†Ô∏è S·ª¨A: D√πng h√†m evaluate_pushup tr·ª±c ti·∫øp
        "state": {"stage": "up", "counter": 0, "prev_angle": 160, "direction": "up"},
    },
    "Plank": {
        "counter_func": count_plank,
        "form_func": evaluate_plank, # üõ†Ô∏è S·ª¨A: D√πng h√†m evaluate_plank tr·ª±c ti·∫øp
        "state": {"good_time": 0, "bad_time": 0, "is_good": False, "start_time": None, "elapsed": 0.0},
    },
    "Sit-up": {
        "counter_func": count_situp,
        "form_func": evaluate_situp, # üõ†Ô∏è S·ª¨A: D√πng h√†m evaluate_situp tr·ª±c ti·∫øp
        "state": {"stage": "down", "counter": 0, "prev_angle": 140, "direction": "down"},
    }
}

# Th√™m h√†m reset_state
def reset_state(exercise_type, state_dict):
    if exercise_type in EXERCISE_REGISTRY:
        state_dict[exercise_type] = EXERCISE_REGISTRY[exercise_type]["state"].copy()
    return state_dict

# --- Logic Webcam Ngo√†i (L·∫•y t·ª´ code c≈©, c·∫≠p nh·∫≠t logic) ---

def _process_frame_logic(frame_bgr, exercise_type, state_dict, prev_time):
    """
    H√†m logic l√µi, d√πng chung cho webcam v√† video.
    X·ª≠ l√Ω 1 frame, tr·∫£ v·ªÅ (annotated_frame, kps, state, fps, prev_time)
    """
    global GLOBAL_MODEL
    model = get_model() # L·∫•y model ƒë√£ kh·ªüi t·∫°o

    # 1. Chu·∫©n b·ªã frame cho inference
    # üõ†Ô∏è S·ª¨A (FIX 4): √âp (squash) frame v·ªÅ 640x640, gi·ªëng h·ªát pose_extractor.py
    frame_in = cv2.resize(frame_bgr, (GLOBAL_IMG_SIZE, GLOBAL_IMG_SIZE), interpolation=cv2.INTER_AREA)

    # 2. Ch·∫°y inference
    try:
        with torch.inference_mode():
            results = model.predict(
                frame_in, # model nh·∫≠n BGR
                verbose=False,
                device=str(device),
                imgsz=GLOBAL_IMG_SIZE,
                half=GLOBAL_USE_HALF,
                max_det=1, # Gi·ªõi h·∫°n 1 ng∆∞·ªùi
                conf=GLOBAL_CONF,
            )
        res = results[0]
        annotated = res.plot() # Tr·∫£ v·ªÅ BGR (ƒë√£ l√† 640x640)
    except Exception as e:
        print(f"L·ªói model predict: {e}")
        res = None
        annotated = frame_in.copy() # D√πng frame_in (640x640) n·∫øu l·ªói

    # 3. Tr√≠ch xu·∫•t KPS
    # üõ†Ô∏è S·ª¨A (FIX 4): L·∫•y KPS tr·ª±c ti·∫øp, kh√¥ng c·∫ßn scale
    kps_scaled = None
    if res:
        kps_scaled_candidate = safe_extract_kps(res) # KPS ƒë√£ ·ªü t·ªça ƒë·ªô 640x640
        if kps_scaled_candidate is not None and kps_scaled_candidate.size > 0:
            kps_scaled = kps_scaled_candidate
    
    # 4. T√≠nh to√°n (Counter & Form)
    # === S·ª¨A L·ªñI (FIX 2): Kh·ªüi t·∫°o bi·∫øn TR∆Ø·ªöC kh·ªëi if/else ===
    current_state = state_dict[exercise_type]
    counter = current_state.get('counter', 0)
    angle = current_state.get('prev_angle', 180.0) # D√πng prev_angle ho·∫∑c angle t·ª´ state
    if exercise_type == "Plank":
        stage_or_good = current_state.get('is_good', False)
        counter = current_state.get('elapsed', 0.0) # counter l√† th·ªùi gian cho plank
    else:
        stage_or_good = current_state.get('stage', 'up') # stage cho c√°c b√†i kh√°c
    
    feedback = "..."
    form_color = (0, 255, 0) # M·∫∑c ƒë·ªãnh l√† 't·ªët' (BGR)
    # === K·∫æT TH√öC S·ª¨A L·ªñI ===

    if kps_scaled is not None:
        try:
            exercise = EXERCISE_REGISTRY[exercise_type]
            counter_func = exercise["counter_func"]
            form_func = exercise["form_func"]
            state = current_state # D√πng state ƒë√£ l·∫•y ·ªü tr√™n

            # H√†m counter_func s·∫Ω c·∫≠p nh·∫≠t 'state' v√† tr·∫£ v·ªÅ gi√° tr·ªã M·ªöI
            result = counter_func(kps_scaled.tolist(), state) 
            
            # === FIX 3: Th√™m kh·ªëi gi·∫£i n√©n (unpack) k·∫øt qu·∫£ ===
            if isinstance(result, (tuple, list)):
                if len(result) == 3:
                    counter, stage_or_good, angle = result
                elif len(result) == 2:
                    counter, stage_or_good = result
                    angle = None # Ho·∫∑c g√°n gi√° tr·ªã m·∫∑c ƒë·ªãnh
                else:
                    counter = result[0]
            else:
                counter = result
            # === K·∫æT TH√öC FIX 3 ===

            # G·ªçi form_func (robust)
            ret = None
            try: 
                # üõ†Ô∏è S·ª¨A (FIX 10): Truy·ªÅn 'state' v√†o form_func (ƒë·ªÉ kh·ªõp v·ªõi form_rules.py ƒê√É S·ª¨A)
                ret = form_func(kps_scaled.tolist(), state, annotated, stage_or_good, counter)
            except Exception as e_form:
                # In l·ªói TypeError n·∫øu form_rules.py CH∆ØA ƒë∆∞·ª£c c·∫≠p nh·∫≠t
                print(f"L·ªói khi g·ªçi form_func (Ki·ªÉm tra form_rules.py): {e_form}")
                ret = None

            if ret is not None and isinstance(ret, tuple) and len(ret) >= 3:
                form_score, feedback, tone = ret
                if tone == "bad": form_color = (0, 0, 255) # X·∫•u (BGR)
                elif tone == "warn": form_color = (0, 165, 255) # Cam (BGR)
            elif ret is not None:
                feedback = str(ret)
                
        except Exception as e:
            print(f"L·ªói counter/form: {e}")
            traceback.print_exc()
            feedback = "L·ªói x·ª≠ l√Ω"

    # 5. T√≠nh FPS
    fps, prev_time = compute_fps(prev_time)

    # 6. Overlay text v·ªõi t√°ch plank/reps
    if font_path is not None:
        if exercise_type == "Plank":
            lines = [
                (f"Th·ªùi gian gi·ªØ: {counter:.1f}s", (255, 215, 0)),
                (f"T∆∞ th·∫ø: {'Chu·∫©n' if stage_or_good else 'Ch∆∞a ƒë√∫ng'}", (255, 255, 255)),
                (f"G√≥c: {int(angle or 0)}¬∞", (144, 238, 144)),
                (f"ƒê√°nh gi√°: {feedback}", form_color), # üõ†Ô∏è S·ª¨A (FIX 9): B·∫≠t l·∫°i d√≤ng n√†y
                (f"FPS: {fps:.1f}", (200, 200, 200)),
            ]
        else:
            lines = [
                (f"S·ªë l·∫ßn: {counter}", (255, 215, 0)),
                (f"Tr·∫°ng th√°i: {stage_or_good}", (255, 255, 255)),
                (f"G√≥c: {int(angle or 0)}¬∞", (144, 238, 144)),
                (f"ƒê√°nh gi√°: {feedback}", form_color), # üõ†Ô∏è S·ª¨A (FIX 9): B·∫≠t l·∫°i d√≤ng n√†y
                (f"FPS: {fps:.1f}", (200, 200, 200)),
            ]
        annotated = draw_text_pil(annotated, lines, font_path=str(font_path), font_scale=26, pos=(20, 20))

    return annotated, kps_scaled, current_state, fps, prev_time

# --- External Webcam Thread (t·ª´ code c≈©) ---

def external_webcam_loop(exercise, weights):
    global EXTERNAL_STOP, EXTERNAL_THREAD
    EXTERNAL_STOP.clear()
    
    try:
        model = get_model(weights) # Warm-up
    except Exception as e:
        print(f"L·ªói t·∫£i model: {e}")
        return "model_error"

    cap = cv2.VideoCapture(CAP_DEVICE_INDEX, cv2.CAP_DSHOW) if os.name=='nt' else cv2.VideoCapture(CAP_DEVICE_INDEX)
    if not cap.isOpened():
        print(f"L·ªói m·ªü webcam index {CAP_DEVICE_INDEX}.")
        return "L·ªói m·ªü webcam."

    # L·∫•y ƒë·ªô ph√¢n gi·∫£i th·ª±c t·∫ø
    ret, sample = cap.read()
    if not ret:
        print("Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ webcam")
        cap.release()
        return "cam_error"
    
    actual_h, actual_w = sample.shape[:2]
    display_w = min(actual_w, DISPLAY_MAX_WIDTH)
    display_h = int(display_w * (actual_h / actual_w))
    
    window_name = "AI Fitness Tracker - Webcam (Nhan 'q' de thoat)"
    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
    cv2.resizeWindow(window_name, display_w, display_h)

    prev_time = time.time()
    
    # üõ†Ô∏è S·ª¨A (FIX 7): Kh·ªüi t·∫°o frame_idx (ƒë·ªïi t·ª´ frame_count)
    frame_idx = 0 
    
    last_annotated = None
    state_dict = {k: v["state"].copy() for k, v in EXERCISE_REGISTRY.items()}
    state_dict = reset_state(exercise, state_dict)  # Reset state

    while not EXTERNAL_STOP.is_set():
        ret, frame = cap.read()
        if not ret:
            break

        # ƒê·ªìng b·ªô v·ªõi INFER_EVERY_N
        if (frame_idx % INFER_EVERY_N) == 0:
            annotated, kps, state_dict[exercise], fps, prev_time = _process_frame_logic(frame, exercise, state_dict, prev_time)
            last_annotated = annotated
            
            # üõ†Ô∏è S·ª¨A (FIX 7): Thay 'frame_count' th√†nh 'frame_idx'
            if (frame_idx % 30) == 0: # Log m·ªói 30 frames
                detected_count = 1 if kps is not None else 0
                print(f"[Webcam] FPS: {fps:.1f} | Detected: {detected_count} | Exercise: {exercise}")

        else:
            annotated = last_annotated if last_annotated is not None else frame.copy()

        # üõ†Ô∏è S·ª¨A (FIX 4): Resize frame 640x640 l√™n k√≠ch th∆∞·ªõc c·ª≠a s·ªï
        if annotated is not None:
            display_frame = cv2.resize(annotated, (display_w, display_h), interpolation=cv2.INTER_AREA)
            cv2.imshow(window_name, display_frame)

        # üõ†Ô∏è S·ª¨A (FIX 7): Thay 'frame_count' th√†nh 'frame_idx'
        frame_idx += 1
        
        # B·ªè logic t·ª± ƒë·ªông d·ª´ng
        # üõ†Ô∏è S·ª¨A (FIX 7): Thay 'frame_count' th√†nh 'frame_idx'
        # if frame_idx > 300:
        #     break

        key = cv2.waitKey(1) & 0xFF
        if key == ord('q') or key == 27:
            EXTERNAL_STOP.set()
            break
        try:
            if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1:
                EXTERNAL_STOP.set()
                break
        except Exception:
             EXTERNAL_STOP.set()
             break

    cap.release()
    cv2.destroyAllWindows()
    EXTERNAL_THREAD = None # Th√™m d√≤ng n√†y
    return "Webcam stopped."

def start_external_webcam_thread(exercise, weights):
    global EXTERNAL_THREAD
    if EXTERNAL_THREAD and EXTERNAL_THREAD.is_alive():
        return "Webcam ƒëang ch·∫°y."
    EXTERNAL_THREAD = threading.Thread(target=external_webcam_loop, args=(exercise, weights), daemon=True)
    EXTERNAL_THREAD.start()
    return "Webcam started (external window)."

def stop_external_webcam_thread():
    global EXTERNAL_STOP, EXTERNAL_THREAD # Th√™m EXTERNAL_THREAD
    EXTERNAL_STOP.set()
    if EXTERNAL_THREAD:
        EXTERNAL_THREAD.join(timeout=5.0)
    return "Webcam stopped."

# --- Video Processing (chia 3 ph·∫ßn, t·ª´ code c≈©) ---

def process_video_split_parts(input_path: str, exercise: str, weights: str, output_resolution=(1920, 1080)):
    global BG_TASK
    
    if not Path(input_path).exists():
        BG_TASK["status"] = "error"
        BG_TASK["error"] = "Input file not found"
        return None, None, None

    cap = cv2.VideoCapture(str(input_path))
    if not cap.isOpened():
        print(f"Kh√¥ng th·ªÉ m·ªü video: {input_path}")
        BG_TASK["status"] = "error"
        BG_TASK["error"] = "cannot_open_video"
        return None, None, None

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0
    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
    
    # üõ†Ô∏è S·ª¨A: L·∫•y k√≠ch th∆∞·ªõc g·ªëc c·ªßa video
    orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    cap.release()

    if total_frames <= 0:
        parts = [(0, None)]
    else:
        per = max(1, total_frames // 3)
        parts = []
        start = 0
        for i in range(3):
            end = start + per - 1 if i < 2 else total_frames - 1
            parts.append((start, end))
            start = end + 1

    # üõ†Ô∏è S·ª¨A: N·∫øu output_resolution m·∫∑c ƒë·ªãnh (1920x1080), 
    # ki·ªÉm tra xem video g·ªëc nh·ªè h∆°n th√¨ d√πng k√≠ch th∆∞·ªõc g·ªëc
    out_w, out_h = output_resolution
    if orig_w > 0 and orig_h > 0:
        # N·∫øu video g·ªëc nh·ªè h∆°n, d√πng k√≠ch th∆∞·ªõc g·ªëc (tr√°nh upscale qu√° nhi·ªÅu)
        if orig_w < out_w or orig_h < out_h:
            out_w, out_h = orig_w, orig_h
            print(f"[VideoProcess] D√πng k√≠ch th∆∞·ªõc g·ªëc ({out_w}x{out_h}) thay v√¨ ({output_resolution[0]}x{output_resolution[1]})")

    tmp_dir = tempfile.mkdtemp(prefix="video_parts_")
    part_paths = [os.path.join(tmp_dir, f"part_{i+1}.mp4") for i in range(len(parts))]
    final_path = os.path.join(tmp_dir, "final_annotated.mp4")

    fourcc = cv2.VideoWriter_fourcc(*"mp4v")

    def process_range(start_frame, end_frame, out_path):
        cap_local = cv2.VideoCapture(str(input_path))
        cap_local.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        writer = cv2.VideoWriter(out_path, fourcc, fps, (out_w, out_h))
        
        # Kh·ªüi t·∫°o state cho process_range
        local_state = {k: v["state"].copy() for k, v in EXERCISE_REGISTRY.items()}
        local_state = reset_state(exercise, local_state)
        prev_time = time.time()
        
        frame_idx = start_frame
        last_log_time = time.time()
        
        last_annotated_canvas = None

        while True:
            if end_frame is not None and frame_idx > end_frame:
                break
            ret, frame = cap_local.read()
            if not ret:
                break
            
            annotated = None
            if (frame_idx % INFER_EVERY_N) == 0:
                try:
                    annotated_orig_size, kps, local_state[exercise], _, prev_time = _process_frame_logic(
                        frame, exercise, local_state, prev_time
                    )
                    
                    # üõ†Ô∏è S·ª¨A: Gi·ªØ t·ªâ l·ªá b·∫±ng letterbox (kh√¥ng k√©o gi√£n)
                    in_h, in_w = annotated_orig_size.shape[:2]
                    scale = min(out_w / in_w, out_h / in_h)
                    new_w = int(in_w * scale)
                    new_h = int(in_h * scale)

                    resized_frame = cv2.resize(annotated_orig_size, (new_w, new_h), interpolation=cv2.INTER_AREA)

                    # Canvas v·ªõi m√†u ƒëen (ho·∫∑c c√≥ th·ªÉ d√πng m√†u kh√°c)
                    canvas = np.zeros((out_h, out_w, 3), dtype=np.uint8)

                    # CƒÉn gi·ªØa frame tr√™n canvas
                    x_offset = (out_w - new_w) // 2
                    y_offset = (out_h - new_h) // 2

                    canvas[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized_frame
                    
                    annotated = canvas
                    last_annotated_canvas = annotated

                except Exception as e:
                    print(f"L·ªói x·ª≠ l√Ω frame {frame_idx}: {e}")
                    traceback.print_exc()
                    try:
                        if 'annotated_orig_size' in locals():
                            annotated = cv2.resize(annotated_orig_size, (out_w, out_h), interpolation=cv2.INTER_AREA)
                        else:
                            annotated = cv2.resize(frame, (out_w, out_h), interpolation=cv2.INTER_AREA)
                    except Exception:
                        annotated = np.zeros((out_h, out_w, 3), dtype=np.uint8)
                    last_annotated_canvas = annotated
            else:
                annotated = last_annotated_canvas

            if annotated is None:
                annotated = np.zeros((out_h, out_w, 3), dtype=np.uint8)
                
            annotated = np.ascontiguousarray(annotated, dtype=np.uint8)
            
            try:
                writer.write(annotated)
            except Exception as e:
                print(f"L·ªói ghi frame {frame_idx}: {e} shape={annotated.shape} dtype={annotated.dtype}")
            
            frame_idx += 1
            
            if time.time() - last_log_time > 5.0:
                print(f"[VideoProcess] ƒê√£ x·ª≠ l√Ω {frame_idx} frames... (ƒêang ·ªü part: {out_path})")
                last_log_time = time.time()

        cap_local.release()
        writer.release()
        print(f"[VideoProcess] ƒê√£ ho√†n th√†nh part: {out_path}")

    # X·ª≠ l√Ω part 1 ƒë·ªìng b·ªô
    print("[VideoProcess] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω Part 1...")
    start0, end0 = parts[0]
    process_range(start0, end0, part_paths[0])
    BG_TASK["part1_path"] = part_paths[0]
    BG_TASK["tmp_dir"] = tmp_dir
    print("[VideoProcess] Ho√†n th√†nh Part 1.")

    def bg_job():
        BG_TASK["status"] = "processing_rest"
        print("[VideoProcess] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω Part 2 & 3 (background)...")
        try:
            for i in range(1, len(parts)):
                s, e = parts[i]
                process_range(s, e, part_paths[i])
            
            print("[VideoProcess] ƒêang n·ªëi c√°c part...")
            out = cv2.VideoWriter(final_path, fourcc, fps, (out_w, out_h))
            for p in part_paths:
                cap_p = cv2.VideoCapture(p)
                while True:
                    ret, frm = cap_p.read()
                    if not ret:
                        break
                    frm = np.ascontiguousarray(frm, dtype=np.uint8)
                    out.write(frm)
                cap_p.release()
            out.release()
            BG_TASK["final_path"] = final_path
            BG_TASK["status"] = "done"
            print("[VideoProcess] ƒê√£ x·ª≠ l√Ω xong video (final).")
        except Exception as e:
            BG_TASK["status"] = "error"
            BG_TASK["error"] = str(e)
            traceback.print_exc()
            print(f"[VideoProcess] L·ªói background: {e}")

    bg_thread = threading.Thread(target=bg_job, daemon=True)
    bg_thread.start()
    BG_TASK["thread"] = bg_thread
    BG_TASK["status"] = "part1_ready"
    return part_paths[0], tmp_dir, bg_thread

# --- Gradio Callbacks (t·ª´ code c≈©) ---
def analyze_video_click(uploaded_file, exercise, weights, resolution):
    if uploaded_file is None:
        return None, "Ch∆∞a t·∫£i file l√™n."
    
    # ƒê·∫£m b·∫£o uploaded_file l√† ƒë∆∞·ªùng d·∫´n
    video_path = uploaded_file
    if hasattr(uploaded_file, 'name'):
        video_path = uploaded_file.name
    elif not isinstance(uploaded_file, str):
         return None, "L·ªói ƒë·ªãnh d·∫°ng file t·∫£i l√™n."

    weights = weights or "yolo11n-pose.pt"
    out_res = (1920, 1080) # Default
    if isinstance(resolution, str):
        try:
            if 'x' in resolution: w, h = map(int, resolution.split('x'))
            elif resolution.endswith('p'): h = int(resolution[:-1]); w = int(h * 16 / 9)
            else: w, h = 1920, 1080
            out_res = (w, h)
        except Exception:
            out_res = (1920, 1080)
            
    BG_TASK["status"] = "starting"
    print(f"[Gradio] B·∫Øt ƒë·∫ßu analyze_video: {video_path} | Ex: {exercise} | Res: {out_res}")
    
    part1, tmpd, thr = process_video_split_parts(video_path, exercise, weights, output_resolution=out_res)
    
    if part1 is None:
        BG_TASK["status"] = "error"
        return None, f"X·ª≠ l√Ω th·∫•t b·∫°i. L·ªói: {BG_TASK.get('error', 'Kh√¥ng r√µ')}"
        
    return part1, f"Part 1 ƒë√£ s·∫µn s√†ng. ƒêang x·ª≠ l√Ω c√°c ph·∫ßn c√≤n l·∫°i... (tmp: {tmpd}). D√πng 'Xem Video (final)' ƒë·ªÉ ki·ªÉm tra."

def view_remaining_click():
    st = BG_TASK.get("status", "idle")
    if st == "done" and BG_TASK.get("final_path"):
        return BG_TASK["final_path"], "Video (final) ƒë√£ s·∫µn s√†ng."
    elif st in ("processing_rest","part1_ready","starting"):
        return None, f"ƒêang x·ª≠ l√Ω (status: {st}). Vui l√≤ng ƒë·ª£i."
    elif st == "error":
        return None, f"L·ªói x·ª≠ l√Ω: {BG_TASK.get('error','unknown')}"
    else:
        return None, "Kh√¥ng c√≥ t√°c v·ª• n√†o ƒëang ch·∫°y."

# Th√™m cleanup
def cleanup():
    if BG_TASK.get("tmp_dir") and os.path.exists(BG_TASK["tmp_dir"]):
        try:
            shutil.rmtree(BG_TASK["tmp_dir"])
            print(f"Cleaned up temp dir: {BG_TASK['tmp_dir']}")
        except Exception as e:
            print(f"Error cleaning up temp dir: {e}")

atexit.register(cleanup)

# --- Giao di·ªán Gradio (t·ª´ code c≈©) ---
DEFAULT_RES_OPTIONS = ["1920x1080", "1280x720", "854x480", "1080p", "720p"]

def build_ui():
    with gr.Blocks(title="AI Fitness Tracker (Merged)") as demo:
        gr.Markdown("# üèãÔ∏è‚Äç‚ôÇÔ∏è AI Fitness Tracker (External Cam + Split Video)")
        gr.Markdown("S·ª≠ d·ª•ng logic `form_rules` m·ªõi. Webcam ch·∫°y ·ªü c·ª≠a s·ªï ngo√†i. Video upload ƒë∆∞·ª£c chia 3 ph·∫ßn.")

        with gr.Row():
            exercise = gr.Dropdown(
                list(EXERCISE_REGISTRY.keys()),
                label="Ch·ªçn b√†i t·∫≠p (ch·ªçn tr∆∞·ªõc khi Start/Analyze)",
                value=list(EXERCISE_REGISTRY.keys())[0]
            )
            weights_input = gr.Textbox(value="yolo11n-pose.pt", label="Model weights path (local file)")

        gr.Markdown("---")
        gr.Markdown("### üé• Webcam Tr·ª±c Ti·∫øp (C·ª≠a s·ªï ngo√†i)")
        with gr.Row():
            start_btn = gr.Button("B·∫Øt ƒë·∫ßu Webcam ngo√†i")
            stop_btn = gr.Button("D·ª´ng Webcam ngo√†i")
        status = gr.Textbox(label="Tr·∫°ng th√°i Webcam", value="S·∫µn s√†ng", interactive=False)
        
        start_btn.click(fn=start_external_webcam_thread, inputs=[exercise, weights_input], outputs=[status])
        stop_btn.click(fn=stop_external_webcam_thread, inputs=None, outputs=[status])

        gr.Markdown("---")
        gr.Markdown("### üìÅ Ph√¢n t√≠ch Video (Chia 3 ph·∫ßn)")
        with gr.Row():
            upload = gr.File(label="T·∫£i video file (.mp4, .mov)")
            # üõ†Ô∏è S·ª¨A (FIX 6): ƒê·∫∑t gi√° tr·ªã m·∫∑c ƒë·ªãnh l√† 1080p (1920x1080)
            res_choice = gr.Dropdown(DEFAULT_RES_OPTIONS, value="1920x1080", label="ƒê·ªô ph√¢n gi·∫£i ƒë·∫ßu ra")
        with gr.Row():
            analyze_btn = gr.Button("üé¨ Ph√¢n t√≠ch Video (Part 1)")
            view_btn = gr.Button("üçø Xem Video (final)")
            
        out_video = gr.Video(
            label="Video k·∫øt qu·∫£ (part 1 ho·∫∑c final)",
            scale=1,  # Cho ph√©p t·ª± ƒëi·ªÅu ch·ªânh theo t·ªâ l·ªá g·ªëc
            format="mp4"
        )
        message = gr.Textbox(label="Tr·∫°ng th√°i Video", value="", interactive=False)

        analyze_btn.click(fn=analyze_video_click, inputs=[upload, exercise, weights_input, res_choice], outputs=[out_video, message])
        view_btn.click(fn=view_remaining_click, inputs=None, outputs=[out_video, message])

    return demo

if __name__ == "__main__":
    app = build_ui()
    try:
        app.launch(server_name="localhost", server_port=7860, share=False)
    except Exception as e:
        print(f"Failed to launch Gradio app: {e}")
        raise
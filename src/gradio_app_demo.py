#!/usr/bin/env python3
"""
gradio_app_demo.py (MERGED + Aspect Ratio Fix + Overlay Fix + Rep Counter Fix)

- N·ªÅn t·∫£ng: Code m·ªõi v·ªõi EXERCISE_REGISTRY, form_rules, v√† utils.
- T√≠nh nƒÉng Webcam: ƒê√£ thay th·∫ø webcam-trong-gradio b·∫±ng webcam-ngo√†i (external OpenCV window)
  t·ª´ code c≈© (external_webcam_loop).
- T√≠nh nƒÉng Video: ƒê√£ thay th·∫ø process_video (all-in-one) b·∫±ng process_video_split_parts (chia 3 ph·∫ßn)
  t·ª´ code c≈©.
- T√≠ch h·ª£p: C·∫£ webcam-ngo√†i v√† video-3-ph·∫ßn ƒë·ªÅu ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t
  ƒë·ªÉ g·ªçi EXERCISE_REGISTRY (counter_func, form_func) v√† d√πng draw_text_pil.
- UI: Giao di·ªán (build_ui) ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ ƒëi·ªÅu khi·ªÉn c√°c t√≠nh nƒÉng m·ªõi n√†y.
- FIX 1: C·∫≠p nh·∫≠t h√†m process_range ƒë·ªÉ gi·ªØ t·ªâ l·ªá khung h√¨nh (aspect ratio)
  c·ªßa video upload, tr√°nh b·ªã k√©o d√£n (stretch) b·∫±ng c√°ch d√πng letterbox/pillarbox.
- FIX 2: Kh·ªüi t·∫°o c√°c bi·∫øn counter, stage, angle *tr∆∞·ªõc* kh·ªëi if/else
  trong _process_frame_logic ƒë·ªÉ tr√°nh l·ªói UnboundLocalError khi kh√¥ng ph√°t hi·ªán ng∆∞·ªùi.
- UPDATE: ƒê·ªìng b·ªô c·∫•u h√¨nh (IMG_SIZE=640, INFER_EVERY_N=3) v√† logic overlay
   t·ª´ pose_extractor.py.
- FIX 3 (R·∫§T QUAN TR·ªåNG): S·ª≠a l·ªói kh√¥ng g√°n (unpack) k·∫øt qu·∫£ (counter, stage, angle)
  tr·∫£ v·ªÅ t·ª´ counter_func trong _process_frame_logic.
- FIX 4 (M·ªöI): S·ª≠a l·ªói resize/scale keypoint. √âp frame v·ªÅ 640x640
  v√† d√πng keypoint tr·ª±c ti·∫øp (gi·ªëng pose_extractor.py).
- FIX 5 (M·ªöI): C·∫≠p nh·∫≠t l·ªánh g·ªçi form_func ƒë·ªÉ truy·ªÅn 'state' (kh·ªõp v·ªõi form_rules.py m·ªõi).
"""

import os
import gradio as gr
from ultralytics import YOLO
import cv2
import torch
import time
import threading
import numpy as np
import tempfile
import traceback
from pathlib import Path
from typing import Optional, Callable
import shutil
import atexit

# === Import logic m·ªõi ===
from rep_counter import count_squat, count_pushup, count_plank, count_situp
# Import per-exercise evaluation functions from form_rules
from form_rules import evaluate_squat, evaluate_pushup, evaluate_plank, evaluate_situp
from utils.draw_utils import draw_text_pil
from utils.video_utils import compute_fps

# === C√°c bi·∫øn to√†n c·ª•c cho Webcam Ngo√†i (t·ª´ code c≈©) ===
EXTERNAL_THREAD = None
EXTERNAL_STOP = threading.Event()
# (MODEL v√† MODEL_LOCK s·∫Ω d√πng chung v·ªõi logic m·ªõi)

# === C√°c bi·∫øn to√†n c·ª•c cho Video 3 ph·∫ßn (t·ª´ code c≈©) ===
BG_TASK = {"thread": None, "status": "idle", "part1_path": None, "final_path": None, "tmp_dir": None, "error": None}

# === C·∫•u h√¨nh chung (t·ª´ code m·ªõi) ===
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    print("=== üîç GPU/CUDA Status ===")
    print(f"CUDA available: {torch.cuda.is_available()}")
    print(f"Current device: {torch.cuda.current_device()}")
    print(f"Device name: {torch.cuda.get_device_name(0)}")
    print(f"Device memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print("=====================")
    # Optimize CUDA backends for real-time
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
else:
    device = torch.device("cpu")
    print("‚ö†Ô∏è CUDA not available. Running on CPU.")

try:
    from moviepy.editor import ImageSequenceClip
    USE_MOVIEPY = True
except Exception:
    ImageSequenceClip = None
    USE_MOVIEPY = False

# Global model and stream state for webcam processing
GLOBAL_MODEL = None
MODEL_LOCK = threading.Lock() # Th√™m lock t·ª´ logic m·ªõi
GLOBAL_CONF = 0.5

# === C·∫¨P NH·∫¨T: ƒê·ªìng b·ªô t·ª´ pose_extractor.py ===
GLOBAL_IMG_SIZE = 640  # K√≠ch th∆∞·ªõc input cho model (thay v√¨ 384)
INFER_EVERY_N = 3 # ƒê·ªìng b·ªô v·ªõi DRAW_EVERY_N_FRAMES (thay v√¨ 2)
# === K·∫æT TH√öC C·∫¨P NH·∫¨T ===

GLOBAL_USE_HALF = bool(torch.cuda.is_available())  # d√πng FP16 n·∫øu c√≥ CUDA
DISPLAY_MAX_WIDTH = 1280 # Cho c·ª≠a s·ªï webcam ngo√†i
CAP_DEVICE_INDEX = 0

# ƒê∆∞·ªùng d·∫´n font (t·ª´ logic m·ªõi)
font_path = Path(__file__).parent.parent / "fonts" / "Roboto.ttf"
if not font_path.exists():
    # Fallback n·∫øu kh√¥ng t√¨m th·∫•y font
    try:
        # Th·ª≠ arial tr√™n Windows
        font_path = "arial.ttf"
        from PIL import ImageFont
        ImageFont.truetype(font_path, 10)
    except Exception:
        # Fallback cu·ªëi c√πng
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y font 'Roboto.ttf' ho·∫∑c 'arial.ttf'. Overlay vƒÉn b·∫£n c√≥ th·ªÉ b·ªã l·ªói.")
        font_path = None


def get_model(weights_path: Optional[str] = "yolo11n-pose.pt"):
    """
    H·ª£p nh·∫•t t·ª´ _get_model (m·ªõi) v√† get_model (c≈©).
    Lazily initialize, d√πng lock, h·ªó tr·ª£ custom weights, v√† warm-up.
    """
    global GLOBAL_MODEL, MODEL_LOCK
    with MODEL_LOCK:
        if GLOBAL_MODEL is None:
            try:
                m = YOLO(weights_path)
            except Exception as e:
                print(f"Error loading {weights_path}: {e}. Fallback to default.")
                m = YOLO("yolo11n-pose.pt")
            m.conf = GLOBAL_CONF
            try:
                m.to(device)
            except Exception:
                pass
            # Fuse Conv+BN ƒë·ªÉ tƒÉng t·ªëc n·∫øu ƒë∆∞·ª£c h·ªó tr·ª£
            try:
                m.fuse()
            except Exception:
                pass
            # Warm-up
            try:
                import numpy as _np
                dummy = _np.zeros((GLOBAL_IMG_SIZE, GLOBAL_IMG_SIZE, 3), dtype=_np.uint8)
                with torch.inference_mode():
                    _ = m.predict(dummy, verbose=False, device=str(device), imgsz=GLOBAL_IMG_SIZE, half=GLOBAL_USE_HALF, max_det=1, conf=GLOBAL_CONF)
            except Exception:
                pass
            GLOBAL_MODEL = m
        return GLOBAL_MODEL

# L·∫•y h√†m tr√≠ch xu·∫•t KPS t·ªët nh·∫•t (t·ª´ code local_webcam trong file m·ªõi)
def safe_extract_kps(res):
    """
    Tr√≠ch xu·∫•t keypoints (robust) t·ª´ k·∫øt qu·∫£ YOLO.
    (ƒê·ªïi t√™n t·ª´ safe_get_kps_from_res_local)
    """
    try:
        r0 = res[0]
        if hasattr(r0, "keypoints") and r0.keypoints is not None:
            if not res or not res.keypoints or not res.boxes:
                return None
            # Ch·ªçn person v·ªõi conf cao nh·∫•t
            confs = res.boxes.conf.cpu().numpy() if res.boxes.conf is not None else []
            if len(confs) == 0:
                return None
            max_idx = np.argmax(confs)
            if confs[max_idx] < 0.6:  # Threshold ƒë·ªÉ b·ªè low-conf
                return None
            kps = res.keypoints.xy[max_idx].tolist()
            return np.array(kps)[:, :2]  # Tr·∫£ array cho d·ªÖ d√πng
    except Exception:
        pass
    return None

# Registry b√†i t·∫≠p (t·ª´ code m·ªõi)
EXERCISE_REGISTRY = {
    "Squat": {
        "counter_func": count_squat,
        "form_func": evaluate_squat, # üõ†Ô∏è S·ª¨A: D√πng h√†m evaluate_squat tr·ª±c ti·∫øp
        "state": {"stage": "up", "counter": 0, "prev_angle": 160, "direction": "up"},
    },
    "Push-up": {
        "counter_func": count_pushup,
        "form_func": evaluate_pushup, # üõ†Ô∏è S·ª¨A: D√πng h√†m evaluate_pushup tr·ª±c ti·∫øp
        "state": {"stage": "up", "counter": 0, "prev_angle": 160, "direction": "up"},
    },
    "Plank": {
        "counter_func": count_plank,
        "form_func": evaluate_plank, # üõ†Ô∏è S·ª¨A: D√πng h√†m evaluate_plank tr·ª±c ti·∫øp
        "state": {"good_time": 0, "bad_time": 0, "is_good": False, "start_time": None, "elapsed": 0.0},
    },
    "Sit-up": {
        "counter_func": count_situp,
        "form_func": evaluate_situp, # üõ†Ô∏è S·ª¨A: D√πng h√†m evaluate_situp tr·ª±c ti·∫øp
        "state": {"stage": "down", "counter": 0, "prev_angle": 140, "direction": "down"},
    }
}

# Th√™m h√†m reset_state
def reset_state(exercise_type, state_dict):
    if exercise_type in EXERCISE_REGISTRY:
        state_dict[exercise_type] = EXERCISE_REGISTRY[exercise_type]["state"].copy()
    return state_dict

# --- Logic Webcam Ngo√†i (L·∫•y t·ª´ code c≈©, c·∫≠p nh·∫≠t logic) ---

def _process_frame_logic(frame_bgr, exercise_type, state_dict, prev_time):
    """
    H√†m logic l√µi, d√πng chung cho webcam v√† video.
    X·ª≠ l√Ω 1 frame, tr·∫£ v·ªÅ (annotated_frame, kps, state, fps, prev_time)
    """
    global GLOBAL_MODEL
    model = get_model() # L·∫•y model ƒë√£ kh·ªüi t·∫°o

    # 1. Chu·∫©n b·ªã frame cho inference
    # üõ†Ô∏è S·ª¨A (FIX 4): √âp (squash) frame v·ªÅ 640x640, gi·ªëng h·ªát pose_extractor.py
    frame_in = cv2.resize(frame_bgr, (GLOBAL_IMG_SIZE, GLOBAL_IMG_SIZE), interpolation=cv2.INTER_AREA)

    # 2. Ch·∫°y inference
    try:
        with torch.inference_mode():
            results = model.predict(
                frame_in, # model nh·∫≠n BGR
                verbose=False,
                device=str(device),
                imgsz=GLOBAL_IMG_SIZE,
                half=GLOBAL_USE_HALF,
                max_det=1,
                conf=GLOBAL_CONF,
            )
        res = results[0]
        annotated = res.plot() # Tr·∫£ v·ªÅ BGR (ƒë√£ l√† 640x640)
    except Exception as e:
        print(f"L·ªói model predict: {e}")
        res = None
        annotated = frame_in.copy() # D√πng frame_in (640x640) n·∫øu l·ªói

    # 3. Tr√≠ch xu·∫•t KPS
    # üõ†Ô∏è S·ª¨A (FIX 4): L·∫•y KPS tr·ª±c ti·∫øp, kh√¥ng c·∫ßn scale
    kps_scaled = None
    if res:
        kps_scaled_candidate = safe_extract_kps(res) # KPS ƒë√£ ·ªü t·ªça ƒë·ªô 640x640
        if kps_scaled_candidate is not None and kps_scaled_candidate.size > 0:
            kps_scaled = kps_scaled_candidate
    
    # 4. T√≠nh to√°n (Counter & Form)
    # === S·ª¨A L·ªñI (FIX 2): Kh·ªüi t·∫°o bi·∫øn TR∆Ø·ªöC kh·ªëi if/else ===
    current_state = state_dict[exercise_type]
    counter = current_state.get('counter', 0)
    angle = current_state.get('prev_angle', 180.0) # D√πng prev_angle ho·∫∑c angle t·ª´ state
    if exercise_type == "Plank":
        stage_or_good = current_state.get('is_good', False)
        counter = current_state.get('elapsed', 0.0) # counter l√† th·ªùi gian cho plank
    else:
        stage_or_good = current_state.get('stage', 'up') # stage cho c√°c b√†i kh√°c
    
    feedback = "..."
    form_color = (0, 255, 0) # M·∫∑c ƒë·ªãnh l√† 't·ªët' (BGR)
    # === K·∫æT TH√öC S·ª¨A L·ªñI ===

    if kps_scaled is not None:
        try:
            exercise = EXERCISE_REGISTRY[exercise_type]
            counter_func = exercise["counter_func"]
            form_func = exercise["form_func"]
            state = current_state # D√πng state ƒë√£ l·∫•y ·ªü tr√™n

            # H√†m counter_func s·∫Ω c·∫≠p nh·∫≠t 'state' v√† tr·∫£ v·ªÅ gi√° tr·ªã M·ªöI
            result = counter_func(kps_scaled.tolist(), state) 
            
            # === FIX 3: Th√™m kh·ªëi gi·∫£i n√©n (unpack) k·∫øt qu·∫£ ===
            if isinstance(result, (tuple, list)):
                if len(result) == 3:
                    counter, stage_or_good, angle = result
                elif len(result) == 2:
                    counter, stage_or_good = result
                    angle = None # Ho·∫∑c g√°n gi√° tr·ªã m·∫∑c ƒë·ªãnh
                else:
                    counter = result[0]
            else:
                counter = result
            # === K·∫æT TH√öC FIX 3 ===

            # G·ªçi form_func (robust)
            ret = None
            try: 
                # üõ†Ô∏è S·ª¨A (FIX 5): Truy·ªÅn 'state' v√†o form_func
                ret = form_func(kps_scaled.tolist(), annotated, stage_or_good, counter)
            except TypeError:
                try: 
                    # Fallback n·∫øu form_func kh√¥ng nh·∫≠n state
                    ret = form_func(kps_scaled.tolist(), annotated, counter)
                except TypeError:
                    try: 
                        ret = form_func(kps_scaled.tolist(), counter)
                    except Exception: ret = None
            except Exception: ret = None

            if ret is not None and isinstance(ret, tuple) and len(ret) >= 3:
                form_score, feedback, tone = ret
                form_color = (0, 255, 0) if tone == "good" else (0, 0, 255)
        except Exception as e:
            print(f"L·ªói counter/form: {e}")
            feedback = "L·ªói x·ª≠ l√Ω"

    # 5. T√≠nh FPS
    fps, prev_time = compute_fps(prev_time)

    # 6. Overlay text v·ªõi t√°ch plank/reps
    if font_path is not None:
        if exercise_type == "Plank":
            lines = [
                (f"Th·ªùi gian gi·ªØ: {counter:.1f}s", (255, 215, 0)),
                (f"T∆∞ th·∫ø: {'Chu·∫©n' if stage_or_good else 'Ch∆∞a ƒë√∫ng'}", (255, 255, 255)),
                (f"G√≥c: {int(angle or 0)}¬∞", (144, 238, 144)),
                (f"ƒê√°nh gi√°: {feedback}", form_color),
                (f"FPS: {fps:.1f}", (200, 200, 200)),
            ]
        else:
            lines = [
                (f"S·ªë l·∫ßn: {counter}", (255, 215, 0)),
                (f"Tr·∫°ng th√°i: {stage_or_good}", (255, 255, 255)),
                (f"G√≥c: {int(angle or 0)}¬∞", (144, 238, 144)),
                (f"ƒê√°nh gi√°: {feedback}", form_color),
                (f"FPS: {fps:.1f}", (200, 200, 200)),
            ]
        annotated = draw_text_pil(annotated, lines, font_path=str(font_path), font_scale=26, pos=(20, 20))

    return annotated, kps_scaled, current_state, fps, prev_time

# --- External Webcam Thread (t·ª´ code c≈©) ---

def external_webcam_loop(exercise, weights):
    global EXTERNAL_STOP
    EXTERNAL_STOP.clear()
    model = get_model(weights) # Warm-up

    cap = cv2.VideoCapture(CAP_DEVICE_INDEX)
    if not cap.isOpened():
        print("L·ªói m·ªü webcam.")
        return "L·ªói m·ªü webcam."

    prev_time = time.time()
    frame_idx = 0
    last_annotated = None
    state_dict = {k: v["state"].copy() for k, v in EXERCISE_REGISTRY.items()}
    state_dict = reset_state(exercise, state_dict)  # Reset state

    while not EXTERNAL_STOP.is_set():
        ret, frame = cap.read()
        if not ret:
            break

        # ƒê·ªìng b·ªô v·ªõi INFER_EVERY_N
        if (frame_idx % INFER_EVERY_N) == 0:
            annotated, _, state_dict[exercise], fps, prev_time = _process_frame_logic(frame, exercise, state_dict, prev_time)
            last_annotated = annotated
        else:
            annotated = last_annotated if last_annotated is not None else frame.copy()

        cv2.imshow("Webcam AI Fitness", annotated)
        frame_idx += 1

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()
    return "Webcam stopped."

def start_external_webcam_thread(exercise, weights):
    global EXTERNAL_THREAD
    if EXTERNAL_THREAD and EXTERNAL_THREAD.is_alive():
        return "Webcam ƒëang ch·∫°y."
    EXTERNAL_THREAD = threading.Thread(target=external_webcam_loop, args=(exercise, weights), daemon=True)
    EXTERNAL_THREAD.start()
    return "Webcam started (external window)."

def stop_external_webcam_thread():
    global EXTERNAL_STOP
    EXTERNAL_STOP.set()
    if EXTERNAL_THREAD:
        EXTERNAL_THREAD.join(timeout=5.0)
    return "Webcam stopped."

# --- Video Processing (chia 3 ph·∫ßn, t·ª´ code c≈©) ---

def process_video_split_parts(input_path: str, exercise: str, weights: str, output_resolution=(1920, 1080)):
    if not Path(input_path).exists():
        return None, None, None

    cap = cv2.VideoCapture(str(input_path))
    if not cap.isOpened():
        print(f"Kh√¥ng th·ªÉ m·ªü video: {input_path}")
        BG_TASK["status"] = "error"
        BG_TASK["error"] = "cannot_open_video"
        return None, None, None

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0
    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
    cap.release() # ƒê√≥ng l·∫°i ngay, s·∫Ω m·ªü l·∫°i trong process_range

    if total_frames <= 0:
        parts = [(0, None)]
    else:
        per = max(1, total_frames // 3)
        parts = []
        start = 0
        for i in range(3):
            end = start + per - 1 if i < 2 else total_frames - 1
            parts.append((start, end))
            start = end + 1

    tmp_dir = tempfile.mkdtemp(prefix="video_parts_")
    part_paths = [os.path.join(tmp_dir, f"part_{i+1}.mp4") for i in range(len(parts))]
    final_path = os.path.join(tmp_dir, "final_annotated.mp4")

    out_w, out_h = output_resolution
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")

    def process_range(start_frame, end_frame, out_path):
        cap_local = cv2.VideoCapture(str(input_path))
        cap_local.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        writer = cv2.VideoWriter(out_path, fourcc, fps, (out_w, out_h))
        
        # Kh·ªüi t·∫°o state cho process_range
        local_state = {k: v["state"].copy() for k, v in EXERCISE_REGISTRY.items()}
        local_state = reset_state(exercise, local_state)  # Reset state
        prev_time = time.time()
        
        frame_idx = start_frame
        last_log_time = time.time()
        
        last_annotated_canvas = None # Cache cho video

        # === B·∫ÆT ƒê·∫¶U V√íNG L·∫∂P ƒê√É S·ª¨A L·ªñI ===
        while True:
            if end_frame is not None and frame_idx > end_frame:
                break
            ret, frame = cap_local.read()
            if not ret:
                break
            
            # === C·∫¨P NH·∫¨T: ƒê·ªìng b·ªô logic INFER_EVERY_N ===
            annotated = None
            if (frame_idx % INFER_EVERY_N) == 0:
                try:
                    # 1. X·ª≠ l√Ω frame, 
                    # üõ†Ô∏è S·ª¨A (FIX 4): 'annotated_orig_size' b√¢y gi·ªù l√† 640x640
                    annotated_orig_size, kps, local_state, _, prev_time = _process_frame_logic(
                        frame, exercise, local_state, prev_time
                    )
                    
                    # === S·ª¨A L·ªñI GI·ªÆ KHUNG H√åNH G·ªêC (LETTERBOX/PILLARBOX) ===
                    
                    # üõ†Ô∏è S·ª¨A (FIX 4): K√≠ch th∆∞·ªõc v√†o l√† 640x640
                    in_h, in_w = annotated_orig_size.shape[:2] # (640, 640)

                    # 2. T√≠nh to√°n t·ªâ l·ªá (scale) ƒë·ªÉ gi·ªØ nguy√™n aspect ratio
                    scale = min(out_w / in_w, out_h / in_h)
                    new_w = int(in_w * scale)
                    new_h = int(in_h * scale)

                    # 3. Resize frame v·ªÅ k√≠ch th∆∞·ªõc m·ªõi (v·∫´n gi·ªØ t·ªâ l·ªá)
                    resized_frame = cv2.resize(annotated_orig_size, (new_w, new_h), interpolation=cv2.INTER_AREA)

                    # 4. T·∫°o canvas ƒëen (k√≠ch th∆∞·ªõc output)
                    canvas = np.zeros((out_h, out_w, 3), dtype=np.uint8)

                    # 5. T√≠nh to√°n v·ªã tr√≠ paste (ƒë·ªÉ cƒÉn gi·ªØa)
                    x_offset = (out_w - new_w) // 2
                    y_offset = (out_h - new_h) // 2

                    # 6. Paste frame ƒë√£ resize v√†o canvas
                    canvas[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized_frame
                    
                    annotated = canvas # Frame cu·ªëi c√πng ƒë·ªÉ ghi l√† canvas
                    last_annotated_canvas = annotated # Cache l·∫°i
                    # === K·∫æT TH√öC S·ª¨A L·ªñI ===

                except Exception as e:
                    print(f"L·ªói x·ª≠ l√Ω frame {frame_idx}: {e}")
                    # Fallback: V·∫´n t·∫°o canvas ƒëen v√† resize (c√≥ th·ªÉ b·ªã m√©o)
                    try:
                        # üõ†Ô∏è S·ª¨A (FIX 4): Resize frame 640x640 b·ªã l·ªói
                        if 'annotated_orig_size' in locals():
                             annotated = cv2.resize(annotated_orig_size, (out_w, out_h), interpolation=cv2.INTER_AREA)
                        else:
                             annotated = cv2.resize(frame, (out_w, out_h), interpolation=cv2.INTER_AREA)
                    except Exception:
                        annotated = np.zeros((out_h, out_w, 3), dtype=np.uint8) # Fallback cu·ªëi
                    last_annotated_canvas = annotated
            else:
                # D√πng frame ƒë√£ cache
                annotated = last_annotated_canvas

            if annotated is None: # X·ª≠ l√Ω frame ƒë·∫ßu ti√™n
                annotated = np.zeros((out_h, out_w, 3), dtype=np.uint8)
                
            # ƒê·∫£m b·∫£o ƒë√∫ng dtype (kh√¥ng c·∫ßn check size n·ªØa v√¨ ƒë√£ t·∫°o canvas)
            annotated = np.ascontiguousarray(annotated, dtype=np.uint8)
            
            try:
                writer.write(annotated) # Frame ƒë√£ l√† BGR
            except Exception as e:
                print(f"L·ªói ghi frame {frame_idx}: {e} shape={annotated.shape} dtype={annotated.dtype}")
            
            frame_idx += 1
            
            # Log ra terminal (logic t·ª´ code m·ªõi)
            if time.time() - last_log_time > 5.0:
                print(f"[VideoProcess] ƒê√£ x·ª≠ l√Ω {frame_idx} frames... (ƒêang ·ªü part: {out_path})")
                last_log_time = time.time()
        # === K·∫æT TH√öC V√íNG L·∫∂P ===

        cap_local.release()
        writer.release()
        print(f"[VideoProcess] ƒê√£ ho√†n th√†nh part: {out_path}")

    # X·ª≠ l√Ω part 1 ƒë·ªìng b·ªô
    print("[VideoProcess] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω Part 1...")
    start0, end0 = parts[0]
    process_range(start0, end0, part_paths[0])
    BG_TASK["part1_path"] = part_paths[0]
    BG_TASK["tmp_dir"] = tmp_dir
    print("[VideoProcess] Ho√†n th√†nh Part 1.")

    def bg_job():
        BG_TASK["status"] = "processing_rest"
        print("[VideoProcess] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω Part 2 & 3 (background)...")
        try:
            for i in range(1, len(parts)):
                s,e = parts[i]
                process_range(s, e, part_paths[i])
            
            print("[VideoProcess] ƒêang n·ªëi c√°c part...")
            # N·ªëi c√°c part l·∫°i
            out = cv2.VideoWriter(final_path, fourcc, fps, (out_w, out_h))
            for p in part_paths:
                cap_p = cv2.VideoCapture(p)
                while True:
                    ret, frm = cap_p.read()
                    if not ret:
                        break
                    # (Kh√¥ng c·∫ßn resize/pad n·ªØa v√¨ process_range ƒë√£ x·ª≠ l√Ω)
                    frm = np.ascontiguousarray(frm, dtype=np.uint8)
                    out.write(frm)
                cap_p.release()
            out.release()
            BG_TASK["final_path"] = final_path
            BG_TASK["status"] = "done"
            print("[VideoProcess] ƒê√£ x·ª≠ l√Ω xong video (final).")
        except Exception as e:
            BG_TASK["status"] = "error"
            BG_TASK["error"] = str(e)
            traceback.print_exc()
            print(f"[VideoProcess] L·ªói background: {e}")

    bg_thread = threading.Thread(target=bg_job, daemon=True)
    bg_thread.start()
    BG_TASK["thread"] = bg_thread
    BG_TASK["status"] = "part1_ready"
    return part_paths[0], tmp_dir, bg_thread

# --- Gradio Callbacks (t·ª´ code c≈©) ---
def analyze_video_click(uploaded_file, exercise, weights, resolution):
    if uploaded_file is None:
        return None, "Ch∆∞a t·∫£i file l√™n."
    
    # ƒê·∫£m b·∫£o uploaded_file l√† ƒë∆∞·ªùng d·∫´n
    video_path = uploaded_file
    if hasattr(uploaded_file, 'name'):
        video_path = uploaded_file.name
    elif not isinstance(uploaded_file, str):
         return None, "L·ªói ƒë·ªãnh d·∫°ng file t·∫£i l√™n."

    weights = weights or "yolo11n-pose.pt"
    out_res = (1920, 1080) # Default
    if isinstance(resolution, str):
        try:
            if 'x' in resolution: w, h = map(int, resolution.split('x'))
            elif resolution.endswith('p'): h = int(resolution[:-1]); w = int(h * 16 / 9)
            else: w, h = 1920, 1080
            out_res = (w, h)
        except Exception:
            out_res = (1920, 1080)
            
    BG_TASK["status"] = "starting"
    print(f"[Gradio] B·∫Øt ƒë·∫ßu analyze_video: {video_path} | Ex: {exercise} | Res: {out_res}")
    
    part1, tmpd, thr = process_video_split_parts(video_path, exercise, weights, output_resolution=out_res)
    
    if part1 is None:
        BG_TASK["status"] = "error"
        return None, "X·ª≠ l√Ω th·∫•t b·∫°i (l·ªói model/load video)."
        
    return part1, f"Part 1 ƒë√£ s·∫µn s√†ng. ƒêang x·ª≠ l√Ω c√°c ph·∫ßn c√≤n l·∫°i... (tmp: {tmpd}). D√πng 'Xem Video (final)' ƒë·ªÉ ki·ªÉm tra."

def view_remaining_click():
    st = BG_TASK.get("status", "idle")
    if st == "done" and BG_TASK.get("final_path"):
        return BG_TASK["final_path"], "Video (final) ƒë√£ s·∫µn s√†ng."
    elif st in ("processing_rest","part1_ready","starting"):
        return None, f"ƒêang x·ª≠ l√Ω (status: {st}). Vui l√≤ng ƒë·ª£i."
    elif st == "error":
        return None, f"L·ªói x·ª≠ l√Ω: {BG_TASK.get('error','unknown')}"
    else:
        return None, "Kh√¥ng c√≥ t√°c v·ª• n√†o ƒëang ch·∫°y."

# Th√™m cleanup
def cleanup():
    if BG_TASK["tmp_dir"] and os.path.exists(BG_TASK["tmp_dir"]):
        shutil.rmtree(BG_TASK["tmp_dir"])
        print("Cleaned up temp dir.")

atexit.register(cleanup)

# --- Giao di·ªán Gradio (t·ª´ code c≈©) ---
DEFAULT_RES_OPTIONS = ["1920x1080", "1280x720", "854x480", "1080p", "720p"]

def build_ui():
    with gr.Blocks(title="AI Fitness Tracker (Merged)") as demo:
        gr.Markdown("# üèãÔ∏è‚Äç‚ôÇÔ∏è AI Fitness Tracker (External Cam + Split Video)")
        gr.Markdown("S·ª≠ d·ª•ng logic `form_rules` m·ªõi. Webcam ch·∫°y ·ªü c·ª≠a s·ªï ngo√†i. Video upload ƒë∆∞·ª£c chia 3 ph·∫ßn.")

        with gr.Row():
            exercise = gr.Dropdown(
                list(EXERCISE_REGISTRY.keys()),
                label="Ch·ªçn b√†i t·∫≠p (ch·ªçn tr∆∞·ªõc khi Start/Analyze)",
                value=list(EXERCISE_REGISTRY.keys())[0]
            )
            weights_input = gr.Textbox(value="yolo11n-pose.pt", label="Model weights path (local file)")

        gr.Markdown("---")
        gr.Markdown("### üé• Webcam Tr·ª±c Ti·∫øp (C·ª≠a s·ªï ngo√†i)")
        with gr.Row():
            start_btn = gr.Button("B·∫Øt ƒë·∫ßu Webcam ngo√†i")
            stop_btn = gr.Button("D·ª´ng Webcam ngo√†i")
        status = gr.Textbox(label="Tr·∫°ng th√°i Webcam", value="S·∫µn s√†ng", interactive=False)
        
        start_btn.click(fn=start_external_webcam_thread, inputs=[exercise, weights_input], outputs=[status])
        stop_btn.click(fn=stop_external_webcam_thread, inputs=None, outputs=[status])

        gr.Markdown("---")
        gr.Markdown("### üìÅ Ph√¢n t√≠ch Video (Chia 3 ph·∫ßn)")
        with gr.Row():
            upload = gr.File(label="T·∫£i video file (.mp4, .mov)")
            res_choice = gr.Dropdown(DEFAULT_RES_OPTIONS, value="1280x720", label="ƒê·ªô ph√¢n gi·∫£i ƒë·∫ßu ra")
        with gr.Row():
            analyze_btn = gr.Button("üé¨ Ph√¢n t√≠ch Video (Part 1)")
            view_btn = gr.Button("üçø Xem Video (final)")
            
        out_video = gr.Video(label="Video k·∫øt qu·∫£ (part 1 ho·∫∑c final)")
        message = gr.Textbox(label="Tr·∫°ng th√°i Video", value="", interactive=False)

        analyze_btn.click(fn=analyze_video_click, inputs=[upload, exercise, weights_input, res_choice], outputs=[out_video, message])
        view_btn.click(fn=view_remaining_click, inputs=None, outputs=[out_video, message])

    return demo

if __name__ == "__main__":
    app = build_ui()
    try:
        app.launch(server_name="localhost", server_port=7860, share=False)
    except Exception as e:
        print(f"Failed to launch Gradio app: {e}")
        raise